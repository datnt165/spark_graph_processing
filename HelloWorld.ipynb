{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81750b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biopython\n",
    "# !pip install findspark\n",
    "# !pip install py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c31fa",
   "metadata": {},
   "source": [
    "## Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c800171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from Bio import SeqIO\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dc3cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import itertools as it\n",
    "import gc\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "from collections import namedtuple\n",
    "date_strftime_format = '%Y-%m-%y %H:%M:%S'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73cbd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import Column\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94a245",
   "metadata": {},
   "source": [
    "## Define Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d38e9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_MER = 20\n",
    "BUILD_GRAPH = 'SPARK_MAP' # 'ORIGINAL'\n",
    "LABEL_GRAPH = 'LABEL_PREGEL' # 'LABEL_PREGEL'\n",
    "PREGEL = 'NAIVE' # 'CENTRAL_DEGREE' 'NAIVE' 'LPA' \n",
    "COMPONENT = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba57687",
   "metadata": {},
   "source": [
    "### Log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd8fa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log4j:\n",
    "    def __init__(self, spark):\n",
    "        root_class = \"guru.learningjournal.spark.examples\"\n",
    "        conf = spark.sparkContext.getConf()\n",
    "        app_name = conf.get(\"spark.app.name\")\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        self.logger = log4j.LogManager.getLogger(root_class + '.' + app_name)\n",
    "    \n",
    "    def warn(self, message):\n",
    "        self.logger.warn(message)\n",
    "        logging.warn(message)\n",
    "        \n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "        logging.info(message)\n",
    "    \n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "        logging.error(message)\n",
    "    \n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "        logging.debug(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dedfb",
   "metadata": {},
   "source": [
    "## Initial SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1b93035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Hello Spark').master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile('/Users/DELL/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar')\n",
    "logger = Log4j(spark)\n",
    "from graphframes import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c53f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56452cf3",
   "metadata": {},
   "source": [
    "## Ultility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d688ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_reads(filename, type='fasta'):\n",
    "    try:\n",
    "        seqs = list(SeqIO.parse(filename, type))\n",
    "\n",
    "        reads = []\n",
    "        labels = []\n",
    "\n",
    "        # Detect for paired-end or single-end reads\n",
    "        # If the id of two first reads are different (e.g.: .1 and .2), they are paired-end reads\n",
    "        is_paired_end = False\n",
    "        if len(seqs) > 2 and seqs[0].id[-1:] != seqs[1].id[-1:]:\n",
    "            is_paired_end = True\n",
    "\n",
    "        label_list = dict()\n",
    "        label_index = 0\n",
    "        for i in range(0, len(seqs), 2 if is_paired_end else 1):\n",
    "            read, label = format_read(seqs[i])\n",
    "            if is_paired_end:\n",
    "                read2, label2 = format_read(seqs[i + 1])\n",
    "                read += read2\n",
    "            reads += [str(read)]\n",
    "        \n",
    "            # Create labels\n",
    "            if label not in label_list:\n",
    "                label_list[label] = label_index\n",
    "                label_index += 1\n",
    "            labels.append(label_list[label])\n",
    "        \n",
    "        del seqs\n",
    "        return reads, labels\n",
    "    except:\n",
    "        print('Error when loading file {} '.format(filename))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1948ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_read(read):\n",
    "    # Return sequence and label\n",
    "    z = re.split('[|={,]+', read.description)\n",
    "    return read.seq, z[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fe59f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads, labels = load_meta_reads('data/S1.fna', type='fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd5bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i].append(value2[i])\n",
    "            else:\n",
    "                value1[i] = [value2[i]]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd415f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEdgeParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i] += value2[i]\n",
    "            else:\n",
    "                value1[i] = value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbb676b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_origin():\n",
    "    logging.info('Start 1')\n",
    "    lmers_dict = dict()\n",
    "    for idx, r in enumerate(reads):\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            if lmer in lmers_dict:\n",
    "                lmers_dict[lmer] += [idx]\n",
    "            else:\n",
    "                lmers_dict[lmer] = [idx]\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    print('Adding nodes...')\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "                6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "                12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    for i in range(0, len(labels)):\n",
    "        G.add_node(i, label=labels[i], color=color_map[labels[i]])\n",
    "\n",
    "    print('Adding edges...')\n",
    "    for kv in E_Filtered.items():\n",
    "        G.add_edge(kv[0][0], kv[0][1], weight=kv[1])\n",
    "    print('Graph constructed!')\n",
    "    logging.info('End 1')\n",
    "    return G\n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "#     print(len(E_Filtered.keys()))\n",
    "#     print(lmers_dict[ATAAATACCTTCATTTAATA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8b84492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_map(readsRDD, spark):\n",
    "    logging.info('Start 2')\n",
    "    def create_lmers_pos(tuple):\n",
    "        idx, r = tuple\n",
    "        lmers_dict =list()\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict.append((lmer, idx))\n",
    "#         print(lmers_dict)\n",
    "        return lmers_dict\n",
    "    def create_edge(x):\n",
    "        lmer, idx = x\n",
    "#         print(lmer, idx)\n",
    "        global edge_dict\n",
    "        E=dict()\n",
    "        for e in it.combinations(idx,2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "        edge_dict += E\n",
    "    logging.info(\"Building hash table...\")\n",
    "    lmers_dict = readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).groupByKey().mapValues(list).filter(lambda x: len(x[1]) > 2)\n",
    "    logging.info('Build edge ...')\n",
    "    lmers_dict.coalesce(20).foreach(create_edge)\n",
    "    # Step 1: map\n",
    "        # ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]; ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]\n",
    "    # Step 3: flat\n",
    "        # ['asdasd': [2, 4, 90000], 'asdasdgg': 3]\n",
    "    #     res = readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).reduceByKey(lambda x,y: x.append(y)).count()\n",
    "    global edge_dict\n",
    "    E = edge_dict.value\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "            6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "            12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    logging.info('Add nodes ...')\n",
    "    vertices = spark.createDataFrame([(i, labels[i], color_map[labels[i]]) for i in range(0, len(labels))], ['id', 'colorId', 'color'])\n",
    "    logging.info('Add edges ...')\n",
    "    edges_data = []\n",
    "    for kv in E_Filtered.items():\n",
    "        edges_data += [(kv[0][0], kv[0][1], kv[1])]\n",
    "        edges_data += [(kv[0][1], kv[0][0], kv[1])]\n",
    "    edges = spark.createDataFrame(edges_data, ['src', 'dst', 'numOfLmers'])\n",
    "    vertices = vertices.persist()\n",
    "    edges = edges.persist()\n",
    "    logging.info('Building graph ...')\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    return g\n",
    "    logging.info('End 2')\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e35d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-22 15:38:11 Start 2\n",
      "2022-12-22 15:38:11 Building hash table...\n",
      "2022-12-22 15:38:11 Build edge ...\n",
      "2022-12-22 15:40:31 Add nodes ...\n",
      "2022-12-22 15:40:33 Add edges ...\n",
      "2022-12-22 15:40:50 Building graph ...\n",
      "96367\n",
      "703416\n"
     ]
    }
   ],
   "source": [
    "# def build_overlap_graph( reads, spark ):\n",
    "\n",
    "edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())\n",
    "\n",
    "readsRDD = spark.sparkContext.parallelize(enumerate(reads)).repartition(40).cache()\n",
    "\n",
    "G = build_dict_spark_map(readsRDD,spark) if BUILD_GRAPH == 'SPARK_MAP' else build_dict_origin()\n",
    "\n",
    "print(G.vertices.count())\n",
    "print(G.edges.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf77c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df64ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# edge_dict.aid()\n",
    "print(edge_dict.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afa87d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = G.labelPropagation(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee4eb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "GL = LB.groupBy('label').agg(collect_list('id').alias('group')).select('group').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca84dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = [gl[0] for gl in GL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e00505bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(GROUP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb71af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'w') as filehandle:\n",
    "    for listitem in GROUP:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b8f9999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 12:37:37 Connected Components Algorithm ...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Connected Components Algorithm ...')\n",
    "sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "CC = G.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf04de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|component|count|\n",
      "+---------+-----+\n",
      "|       47| 1600|\n",
      "|       35| 1183|\n",
      "|      111|  944|\n",
      "|       48|  663|\n",
      "|       40|  660|\n",
      "|       58|  631|\n",
      "|       16|  587|\n",
      "|        5|  572|\n",
      "|       54|  564|\n",
      "|       14|  471|\n",
      "|       24|  431|\n",
      "|      461|  425|\n",
      "|      141|  423|\n",
      "|       70|  417|\n",
      "|      152|  412|\n",
      "|       25|  392|\n",
      "|      125|  389|\n",
      "|       23|  388|\n",
      "|        0|  379|\n",
      "|       30|  371|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC.groupBy('component').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65ac912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+\n",
      "|   id|colorId|color|initialId|\n",
      "+-----+-------+-----+---------+\n",
      "|40795|      0|  red|        8|\n",
      "|27321|      0|  red|        7|\n",
      "|23531|      0|  red|        6|\n",
      "|16852|      0|  red|        5|\n",
      "|13484|      0|  red|        4|\n",
      "|11086|      0|  red|        3|\n",
      "| 7323|      0|  red|        2|\n",
      "| 4385|      0|  red|        1|\n",
      "|   30|      0|  red|     null|\n",
      "|   31|      0|  red|     null|\n",
      "|  679|      0|  red|     null|\n",
      "| 1156|      0|  red|     null|\n",
      "|  541|      0|  red|     null|\n",
      "|  923|      0|  red|     null|\n",
      "|  957|      0|  red|     null|\n",
      "| 1277|      0|  red|     null|\n",
      "| 1294|      0|  red|     null|\n",
      "| 1769|      0|  red|     null|\n",
      "| 1110|      0|  red|     null|\n",
      "| 1588|      0|  red|     null|\n",
      "+-----+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices = CC.filter(col('component') == COMPONENT).drop('component')\n",
    "\n",
    "if LABEL_GRAPH == 'LABEL_PREGEL':\n",
    "    num_vertices = vertices.count()\n",
    "    if PREGEL == 'CENTRAL_DEGREE':\n",
    "        out_degrees = G.outDegrees\n",
    "        vertices = vertices.join(out_degrees, ['id'], 'left').orderBy('outDegree', ascending=False).withColumn('row1', row_number().over(Window.orderBy(col(\"outDegree\").desc())))\n",
    "        initialVertice = spark.createDataFrame([[sample] for sample in range(1, int(num_vertices/10) + 1)], ['initialID'])\n",
    "        vertices = vertices.join(initialVertice, vertices['row1'] == initialVertice['initialID'], 'left').drop('row1')\n",
    "    if PREGEL == 'NAIVE':\n",
    "        vertices = vertices.withColumn('row1', row_number().over(Window.orderBy(\"id\")))\n",
    "        initialVertice = spark.createDataFrame([[sample] for sample in random.sample(range(1, num_vertices), int(num_vertices/50) + 1)], ['initialVertice']).withColumn('initialId', row_number().over(Window.orderBy(\"initialVertice\")))\n",
    "        vertices = vertices.join(initialVertice, vertices['row1'] == initialVertice['initialVertice'], 'left').drop('row1','initialVertice')\n",
    "    vertices.cache()\n",
    "    vertices.orderBy('initialID', ascending=False).show()\n",
    "\n",
    "vertices.toPandas().to_json('vertices.json', orient='records')\n",
    "    \n",
    "edges = G.edges.join(vertices,G.edges.src ==  vertices.id,\"leftsemi\")\n",
    "edges.cache()\n",
    "edges.toPandas().to_json('edges.json', orient='records')\n",
    "\n",
    "subGraph = GraphFrame(vertices, edges)\n",
    "# if LABEL_GRAPH == 'LABEL_SPARK_FUNCTION':\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672b6bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|  src|  dst|numOfLmers|\n",
      "+-----+-----+----------+\n",
      "|11913|   30|        34|\n",
      "|   30|11913|        34|\n",
      "|11913|14452|        58|\n",
      "|14452|11913|        58|\n",
      "|11913| 5044|        25|\n",
      "| 5044|11913|        25|\n",
      "|   30|14452|        38|\n",
      "|14452|   30|        38|\n",
      "|   30| 5044|        54|\n",
      "| 5044|   30|        54|\n",
      "|14452| 5044|        29|\n",
      "| 5044|14452|        29|\n",
      "|11913|30305|        56|\n",
      "|30305|11913|        56|\n",
      "|11913|34464|        34|\n",
      "|34464|11913|        34|\n",
      "|   30|30305|        29|\n",
      "|30305|   30|        29|\n",
      "|   30|34464|        21|\n",
      "|34464|   30|        21|\n",
      "+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "2572\n",
      "+----+-------+-----+---------+\n",
      "|  id|colorId|color|initialId|\n",
      "+----+-------+-----+---------+\n",
      "|  30|      0|  red|     null|\n",
      "|  31|      0|  red|     null|\n",
      "| 541|      0|  red|     null|\n",
      "| 679|      0|  red|     null|\n",
      "| 923|      0|  red|     null|\n",
      "| 957|      0|  red|     null|\n",
      "|1110|      0|  red|     null|\n",
      "|1156|      0|  red|     null|\n",
      "|1277|      0|  red|     null|\n",
      "|1294|      0|  red|     null|\n",
      "|1588|      0|  red|     null|\n",
      "|1769|      0|  red|     null|\n",
      "|1945|      0|  red|     null|\n",
      "|2030|      0|  red|     null|\n",
      "|2096|      0|  red|     null|\n",
      "|2108|      0|  red|     null|\n",
      "|2241|      0|  red|     null|\n",
      "|2276|      0|  red|     null|\n",
      "|2463|      0|  red|     null|\n",
      "|2613|      0|  red|     null|\n",
      "+----+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "edges.show()\n",
    "print(edges.count())\n",
    "vertices.show()\n",
    "print(vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bca4a1",
   "metadata": {},
   "source": [
    "## Pregel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c16d0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexProgram(msg, id):\n",
    "    print('Msg', msg, 'ID', id, sep=', ')\n",
    "    return msg\n",
    "vertexProgramUdf = udf(vertexProgram)\n",
    "\n",
    "def sendMsgToDst(srcID, dstID, src, dst):\n",
    "    print('Send', 'src', src, 'dst', dst, srcID, dstID, sep = ', ')\n",
    "    if srcID == None or dstID != None:\n",
    "        return None\n",
    "    else:\n",
    "        return srcID\n",
    "sendMsgToDstUdf = udf(sendMsgToDst)\n",
    "\n",
    "def most_frequent(List):\n",
    "    dict = {}\n",
    "    count, itm = 0, ''\n",
    "    for item in reversed(List):\n",
    "        dict[item] = dict.get(item, 0) + 1\n",
    "        if dict[item] >= count :\n",
    "            count, itm = dict[item], item\n",
    "    return itm\n",
    "\n",
    "def aggMsgs(msg, id):\n",
    "    print('Agg', id, msg,  sep = ', ')\n",
    "\n",
    "    res = most_frequent(msg)\n",
    "    dictionary = {\n",
    "        \"id\": id,\n",
    "        \"label\": res,\n",
    "    }\n",
    "\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "\n",
    "    with open(\"sample.json\", \"a\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        outfile.write(',\\n')\n",
    "    return res\n",
    "aggMsgsUdf = udf(aggMsgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1309985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABEL_GRAPH == 'LABEL_SPARK_FUNCTION':\n",
    "    LB = subGraph.labelPropagation(50)#\n",
    "    LB.drop('colorId', 'color').toPandas().to_json('lb_function.json', orient='records')\n",
    "if LABEL_GRAPH == 'LABEL_PREGEL':\n",
    "    with open(\"sample.json\", \"w\") as outfile:\n",
    "        outfile.write('[')\n",
    "    \n",
    "    sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "    LB = subGraph.pregel \\\n",
    "        .withVertexColumn(\"partitionID\", col('initialID'), coalesce(Pregel.msg(), col('partitionID'))) \\\n",
    "        .sendMsgToDst(when(Pregel.dst('partitionID').isNull() & Pregel.src('partitionID').isNotNull(), Pregel.src('partitionID'))) \\\n",
    "        .aggMsgs(aggMsgsUdf(collect_list(Pregel.msg()),col('id')))  \\\n",
    "        .run()\n",
    "    \n",
    "    with open(\"sample.json\", \"a\") as outfile:\n",
    "        outfile.write(']')\n",
    "#.setMaxIter(10) \\\n",
    "# .sendMsgToDst(sendMsgToDstUdf(Pregel.src('partitionID'),Pregel.dst('partitionID'),Pregel.src('id'),Pregel.dst('id'))) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b48ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c982d5",
   "metadata": {},
   "source": [
    "## Test labelPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c963748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionID|count|\n",
      "+-----------+-----+\n",
      "|          3|   81|\n",
      "|          5|   67|\n",
      "|          1|   52|\n",
      "|          6|   49|\n",
      "|          4|   43|\n",
      "|          8|   40|\n",
      "|          2|   21|\n",
      "|          7|   18|\n",
      "+-----------+-----+\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "count_group = LB.groupBy('partitionID').count().orderBy('count', ascending=False)\n",
    "count_group.show()\n",
    "print(count_group.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "982208d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-----+\n",
      "|   id|colorId|color|label|\n",
      "+-----+-------+-----+-----+\n",
      "| 3091|      0|  red|10912|\n",
      "|  541|      0|  red| 2712|\n",
      "| 1277|      0|  red|22217|\n",
      "|25759|      0|  red|22217|\n",
      "|31111|      0|  red|29179|\n",
      "| 2030|      0|  red| 2030|\n",
      "| 3452|      0|  red|29292|\n",
      "|36890|      0|  red|41458|\n",
      "|11638|      0|  red|29179|\n",
      "|31212|      0|  red|35462|\n",
      "|12860|      0|  red|15470|\n",
      "|12889|      0|  red| 2712|\n",
      "|14043|      0|  red|36563|\n",
      "|33371|      0|  red|30097|\n",
      "|11913|      0|  red| 6310|\n",
      "|17209|      0|  red|36563|\n",
      "|20854|      0|  red|29292|\n",
      "| 2241|      0|  red| 6310|\n",
      "|23358|      0|  red| 4635|\n",
      "|10967|      0|  red|10975|\n",
      "+-----+-------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LB.show()#.drop('color', 'colorId').toPandas().to_json('label.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB.orderBy('initialID', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB.groupby('partitionID').count().sort(desc(\"partitionID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ecb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(lmers_dict_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB.groupBy('label').count().orderBy('count', ascending=False).show()\n",
    "LB.filter(col('label') == '34101').show()\n",
    "LB.select(countDistinct(\"label\")).show()\n",
    "CC.groupBy('component').count().orderBy('count', ascending=False).write.csv(\"componnent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e97277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_test['ATAAATACCTTCATTTAATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "# #     sg = spark.createDataFrame\n",
    "#     edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())\n",
    "# #     lmers_dict_3 = sc.accumulator({}, DictParam())\n",
    "# #     lmers_dict_4 = sc.accumulator({}, DictParam())\n",
    "# #     lmers_dict = sc.accumulator({}, DictParam())\n",
    "# #     gc.collect()\n",
    "#     logger.info('Start')\n",
    "# #     dict_test = build_overlap_graph(reads, sc)\n",
    "#     build_overlap_graph(reads, spark)\n",
    "#     logger.info('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45103f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_foreach(readsRDD):\n",
    "    logging.info('Start 3')\n",
    "    def create_dict_foreach(tuple):\n",
    "        idx, r = tuple\n",
    "        global lmers_dict_3\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict_3 += {lmer: idx}\n",
    "    readsRDD.foreach(create_dict_foreach)\n",
    "    global lmers_dict_3\n",
    "    res = lmers_dict_3.value\n",
    "#     print(res['ATAATTGGCAAGTGTTTTAG'])\n",
    "    print(len(res.keys()))\n",
    "    logging.info('End 3')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_mapPartition(readsRDD):\n",
    "    logging.info('Start 4')\n",
    "    def create_dict_mapPartition(partitionData):\n",
    "        lmers_dict = dict()\n",
    "        for idx, r in [*partitionData]:\n",
    "            for j in range(0,len(r)-L_MER+1):\n",
    "                lmer = r[j:j+L_MER]\n",
    "                if lmer in lmers_dict:\n",
    "                    lmers_dict[lmer] += [idx]\n",
    "                else:\n",
    "                    lmers_dict[lmer] = [idx]\n",
    "        yield lmers_dict\n",
    "    def merge_dict(x,y):\n",
    "        for i in y.keys():\n",
    "            if i in x:\n",
    "                x[i] += y[i] \n",
    "            else:\n",
    "                x[i] = y[i]\n",
    "        return x\n",
    "\n",
    "    lmers_dict = readsRDD.mapPartitions(create_dict_mapPartition).reduce(lambda x, y: merge_dict(x,y))\n",
    "    logging.warning('Processing 1')\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "            if e_curr in E:\n",
    "                E[e_curr] += 1\n",
    "            else:\n",
    "                E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "    print(len(E_Filtered.keys()))\n",
    "    \n",
    "#     logging.warning('Processing 2')\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).collect()\n",
    "#     count = 0\n",
    "#     for dict1 in res:\n",
    "#         count += len(dict1.keys())\n",
    "#     print(count)\n",
    "#     logging.warning('End Processing 2')\n",
    "#     def create_edge(dictionary):\n",
    "#         E  = dict()\n",
    "#         for lmer in dictionary:\n",
    "#             for e in it.combinations(dictionary[lmer],2):\n",
    "#                 if e[0]!=e[1]:\n",
    "#                     e_curr=(e[0],e[1])\n",
    "#                 if e_curr in E:\n",
    "#                     E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "#                 else:\n",
    "#                     E[e_curr] = 1\n",
    "#         E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "#         return E_Filtered\n",
    "# #     first = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "# #     print(first)\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "    \n",
    "#     count = 0\n",
    "#     for edge in res:\n",
    "#         count += len(edge.keys())\n",
    "#     print(count)\n",
    "# #     print(res[(49008, 56213)])\n",
    "    logging.info('End 4')\n",
    "#     return res?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
