{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81750b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biopython\n",
    "# !pip install findspark\n",
    "# !pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73cbd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import coalesce, col, lit, sum, when, max, collect_list, min, udf, struct\n",
    "from pyspark.sql.types import *\n",
    "from graphframes.lib import Pregel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50b371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexProgram(vd: Row, msg:Row):\n",
    "    print('Vertex', vd, msg, sep=': ')\n",
    "#     if msg == None: # or float(vd[0]) < float(msg[0]):\n",
    "#         return (vd[0] , vd[1] , vd[2])\n",
    "#     else:\n",
    "#         return (msg[0] , vd[1] , msg[2])\n",
    "\n",
    "\n",
    "vertexProgramUdf = udf(vertexProgram)\n",
    "\n",
    "def sendMsgToDst(src:Row, dst:Row):\n",
    "#     srcDist = float(src[0] )\n",
    "#     dstDist = float(dst[0] )\n",
    "    print('Send', src, dst, sep=': ')\n",
    "#     if srcDist < (dstDist - 1):\n",
    "#         return (srcDist + 1, src[1] , src[2] + ' ' + dst[1])\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "sendMsgToDstUdf = udf(sendMsgToDst)\n",
    "\n",
    "def aggMsgs(agg: [Row]):\n",
    "    return (agg[0] [0] , agg[0] [1] , agg[0] [2])\n",
    "\n",
    "\n",
    "aggMsgsUdf = udf(aggMsgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576c2209",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m root_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     12\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m ])\n\u001b[1;32m---> 16\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpregel\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetMaxIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithVertexColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_node\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertexProgramUdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPregel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendMsgToDst\u001b[49m\u001b[43m(\u001b[49m\u001b[43msendMsgToDstUdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPregel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPregel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdst\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggMsgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggMsgsUdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPregel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\spark-57489bae-d6bc-4646-9178-6ffcad293cae\\userFiles-bcada03b-353e-4d2a-a095-c2706df56d88\\graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar\\graphframes\\lib\\pregel.py:172\u001b[0m, in \u001b[0;36mPregel.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    Runs the defined Pregel algorithm.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    :return: the result vertex DataFrame from the final iteration including both original and additional columns.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# edges = spark.createDataFrame([[2, 1, 1],[1, 2, 1],[2, 4, 1],[3, 6, 1],[4, 1, 1], [5, 2, 1], [5, 3, 1], [5, 6, 1]], [\"src\", \"dst\", 'attr'])\n",
    "# edges.cache()\n",
    "# vertices = spark.createDataFrame([[0], [1], [2], [3], [4], [5], [6]], [\"id\"])\n",
    "# numVertices = vertices.count()\n",
    "# vertices = GraphFrame(vertices, edges).outDegrees\n",
    "# vertices.cache()\n",
    "# graph = GraphFrame(vertices, edges)\n",
    "# # alpha = 0.15\n",
    "# sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "# root_node = 0\n",
    "# schema = StructType([\n",
    "#     StructField(\"dist\", DoubleType(), True),\n",
    "#     StructField(\"name\", StringType(), True),\n",
    "#     StructField(\"path\", StringType(), True)\n",
    "# ])\n",
    "# ranks = graph.pregel \\\n",
    "#     .setMaxIter(3) \\\n",
    "#     .withVertexColumn(\"path\", when(col('id')==(lit(root_node)), struct(lit(0), col('id'), col('id').astype(StringType()))) \\\n",
    "#                       .otherwise(struct(lit(10000), col('id'), lit(''))).cast(schema) \\\n",
    "#                       , vertexProgramUdf(col('path'), Pregel.msg())) \\\n",
    "#     .sendMsgToDst(sendMsgToDstUdf(Pregel.src(\"path\"), Pregel.dst(\"path\"))) \\\n",
    "#     .aggMsgs(aggMsgsUdf(collect_list(Pregel.msg()))) \\\n",
    "#     .run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aeb701",
   "metadata": {},
   "source": [
    "GraphFrame Pregel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfe6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 11:43:54 NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "edges = spark.createDataFrame([[0, 1], [1, 2],[2, 4],[2, 0],[3, 4],[4, 2]], [\"src\", \"dst\"])\n",
    "edges.cache()\n",
    "vertices = spark.createDataFrame([[0], [1], [2], [3], [4]], [\"id\"])\n",
    "numVertices = vertices.count()\n",
    "vertices = GraphFrame(vertices, edges).outDegrees\n",
    "vertices.cache()\n",
    "graph = GraphFrame(vertices, edges)\n",
    "alpha = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e6fabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertexProgram(msg):\n",
    "    print('Vertex','Msg', msg, 'Msg type', type(msg), sep=': ')\n",
    "    return (msg if msg != None else 0) * (1.0 - alpha) + alpha / numVertices\n",
    "\n",
    "\n",
    "vertexProgramUdf = udf(vertexProgram)\n",
    "\n",
    "def sendMsgToDst(srcRank, srcOutDegree):\n",
    "    print('Send','SrcRank', srcRank, 'SrcRank type', type(srcRank), 'SrcOutDegree', srcOutDegree, 'SrcOutDegree type', type(srcOutDegree), sep=': ')\n",
    "    return float(srcRank) / srcOutDegree\n",
    "\n",
    "\n",
    "sendMsgToDstUdf = udf(sendMsgToDst)\n",
    "\n",
    "def aggMsgs(msg: [Row]):\n",
    "    return sum(msg)\n",
    "\n",
    "\n",
    "aggMsgsUdf = udf(aggMsgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdaa020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "ranks = graph.pregel \\\n",
    "    .setMaxIter(5) \\\n",
    "    .withVertexColumn(\"rank\", lit(1.0 / numVertices), vertexProgramUdf(Pregel.msg())) \\\n",
    "    .sendMsgToDst(sendMsgToDstUdf(Pregel.src(\"rank\"),Pregel.src(\"outDegree\"))) \\\n",
    "    .aggMsgs(aggMsgs(Pregel.msg())) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83fdc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------------+\n",
      "| id|outDegree|               rank|\n",
      "+---+---------+-------------------+\n",
      "|  0|        1|0.19903352343750003|\n",
      "|  1|        1|      0.17089228125|\n",
      "|  3|        1|               0.03|\n",
      "|  2|        2|0.37554067187499995|\n",
      "|  4|        1|0.22453352343750002|\n",
      "+---+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8827d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = graph.pregel \\\n",
    " .setMaxIter(5) \\\n",
    "  .withVertexColumn(\"rank\", lit(1.0 / numVertices), \\\n",
    "      coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n",
    " .sendMsgToDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n",
    "   .aggMsgs(sum(Pregel.msg())) \\\n",
    "   .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74795d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------------+\n",
      "| id|outDegree|               rank|\n",
      "+---+---------+-------------------+\n",
      "|  0|        1|0.19903352343750003|\n",
      "|  1|        1|      0.17089228125|\n",
      "|  3|        1|               0.03|\n",
      "|  2|        2|0.37554067187499995|\n",
      "|  4|        1|0.22453352343750002|\n",
      "+---+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c800171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from Bio import SeqIO\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc3cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import itertools as it\n",
    "import gc\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "from collections import namedtuple\n",
    "date_strftime_format = '%Y-%m-%y %H:%M:%S'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8fa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log4j:\n",
    "    def __init__(self, spark):\n",
    "        root_class = \"guru.learningjournal.spark.examples\"\n",
    "        conf = spark.sparkContext.getConf()\n",
    "        app_name = conf.get(\"spark.app.name\")\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        self.logger = log4j.LogManager.getLogger(root_class + '.' + app_name)\n",
    "    \n",
    "    def warn(self, message):\n",
    "        self.logger.warn(message)\n",
    "        logging.warn(message)\n",
    "        \n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "        logging.info(message)\n",
    "    \n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "        logging.error(message)\n",
    "    \n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "        logging.debug(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dedfb",
   "metadata": {},
   "source": [
    "Initial SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b93035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Hello Spark').master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile('/Users/DELL/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar')\n",
    "logger = Log4j(spark)\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d688ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_reads(filename, type='fasta'):\n",
    "    try:\n",
    "        seqs = list(SeqIO.parse(filename, type))\n",
    "\n",
    "        reads = []\n",
    "        labels = []\n",
    "\n",
    "        # Detect for paired-end or single-end reads\n",
    "        # If the id of two first reads are different (e.g.: .1 and .2), they are paired-end reads\n",
    "        is_paired_end = False\n",
    "        if len(seqs) > 2 and seqs[0].id[-1:] != seqs[1].id[-1:]:\n",
    "            is_paired_end = True\n",
    "\n",
    "        label_list = dict()\n",
    "        label_index = 0\n",
    "        for i in range(0, len(seqs), 2 if is_paired_end else 1):\n",
    "            read, label = format_read(seqs[i])\n",
    "            if is_paired_end:\n",
    "                read2, label2 = format_read(seqs[i + 1])\n",
    "                read += read2\n",
    "            reads += [str(read)]\n",
    "        \n",
    "            # Create labels\n",
    "            if label not in label_list:\n",
    "                label_list[label] = label_index\n",
    "                label_index += 1\n",
    "            labels.append(label_list[label])\n",
    "        \n",
    "        del seqs\n",
    "        return reads, labels\n",
    "    except:\n",
    "        print('Error when loading file {} '.format(filename))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1948ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_read(read):\n",
    "    # Return sequence and label\n",
    "    z = re.split('[|={,]+', read.description)\n",
    "    return read.seq, z[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe59f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads, labels = load_meta_reads('data/S1.fna', type='fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36fdb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_MER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd5bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i].append(value2[i])\n",
    "            else:\n",
    "                value1[i] = [value2[i]]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd415f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEdgeParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i] += value2[i]\n",
    "            else:\n",
    "                value1[i] = value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb676b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_origin():\n",
    "    logging.info('Start 1')\n",
    "    lmers_dict = dict()\n",
    "    for idx, r in enumerate(reads):\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            if lmer in lmers_dict:\n",
    "                lmers_dict[lmer] += [idx]\n",
    "            else:\n",
    "                lmers_dict[lmer] = [idx]\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    print('Adding nodes...')\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "                6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "                12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    for i in range(0, len(labels)):\n",
    "        G.add_node(i, label=labels[i], color=color_map[labels[i]])\n",
    "\n",
    "    print('Adding edges...')\n",
    "    for kv in E_Filtered.items():\n",
    "        G.add_edge(kv[0][0], kv[0][1], weight=kv[1])\n",
    "    print('Graph constructed!')\n",
    "    logging.info('End 1')\n",
    "    return G\n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "#     print(len(E_Filtered.keys()))\n",
    "#     print(lmers_dict[ATAAATACCTTCATTTAATA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b84492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_map(readsRDD, spark):\n",
    "    logging.info('Start 2')\n",
    "    def create_lmers_pos(tuple):\n",
    "        idx, r = tuple\n",
    "        lmers_dict =list()\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict.append((lmer, idx))\n",
    "#         print(lmers_dict)\n",
    "        return lmers_dict\n",
    "    def create_edge(x):\n",
    "        lmer, idx = x\n",
    "#         print(lmer, idx)\n",
    "        global edge_dict\n",
    "        E=dict()\n",
    "        for e in it.combinations(idx,2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "        edge_dict += E\n",
    "    logging.info(\"Building hash table...\")\n",
    "    lmers_dict = readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).groupByKey().mapValues(list).filter(lambda x: len(x[1]) > 2)\n",
    "    logging.info('Build edge ...')\n",
    "    lmers_dict.coalesce(20).foreach(create_edge)\n",
    "    # Step 1: map\n",
    "        # ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]; ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]\n",
    "    # Step 3: flat\n",
    "        # ['asdasd': [2, 4, 90000], 'asdasdgg': 3]\n",
    "    #     res = readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).reduceByKey(lambda x,y: x.append(y)).count()\n",
    "    global edge_dict\n",
    "    E = edge_dict.value\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "            6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "            12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    logging.info('Add nodes ...')\n",
    "    vertices = spark.createDataFrame([(i, labels[i], color_map[labels[i]]) for i in range(0, len(labels))], ['id', 'colorId', 'color'])\n",
    "    logging.info('Add edges ...')\n",
    "    edges = spark.createDataFrame([(kv[0][0], kv[0][1], kv[1]) for kv in E_Filtered.items()], ['src', 'dst', 'numOfLmers'])\n",
    "    logging.info('Building graph ...')\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    return g\n",
    "    logging.info('End 2')\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45103f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_foreach(readsRDD):\n",
    "    logging.info('Start 3')\n",
    "    def create_dict_foreach(tuple):\n",
    "        idx, r = tuple\n",
    "        global lmers_dict_3\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict_3 += {lmer: idx}\n",
    "    readsRDD.foreach(create_dict_foreach)\n",
    "    global lmers_dict_3\n",
    "    res = lmers_dict_3.value\n",
    "#     print(res['ATAATTGGCAAGTGTTTTAG'])\n",
    "    print(len(res.keys()))\n",
    "    logging.info('End 3')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b23bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_mapPartition(readsRDD):\n",
    "    logging.info('Start 4')\n",
    "    def create_dict_mapPartition(partitionData):\n",
    "        lmers_dict = dict()\n",
    "        for idx, r in [*partitionData]:\n",
    "            for j in range(0,len(r)-L_MER+1):\n",
    "                lmer = r[j:j+L_MER]\n",
    "                if lmer in lmers_dict:\n",
    "                    lmers_dict[lmer] += [idx]\n",
    "                else:\n",
    "                    lmers_dict[lmer] = [idx]\n",
    "        yield lmers_dict\n",
    "    def merge_dict(x,y):\n",
    "        for i in y.keys():\n",
    "            if i in x:\n",
    "                x[i] += y[i] \n",
    "            else:\n",
    "                x[i] = y[i]\n",
    "        return x\n",
    "\n",
    "    lmers_dict = readsRDD.mapPartitions(create_dict_mapPartition).reduce(lambda x, y: merge_dict(x,y))\n",
    "    logging.warning('Processing 1')\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "            if e_curr in E:\n",
    "                E[e_curr] += 1\n",
    "            else:\n",
    "                E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "    print(len(E_Filtered.keys()))\n",
    "    \n",
    "#     logging.warning('Processing 2')\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).collect()\n",
    "#     count = 0\n",
    "#     for dict1 in res:\n",
    "#         count += len(dict1.keys())\n",
    "#     print(count)\n",
    "#     logging.warning('End Processing 2')\n",
    "#     def create_edge(dictionary):\n",
    "#         E  = dict()\n",
    "#         for lmer in dictionary:\n",
    "#             for e in it.combinations(dictionary[lmer],2):\n",
    "#                 if e[0]!=e[1]:\n",
    "#                     e_curr=(e[0],e[1])\n",
    "#                 if e_curr in E:\n",
    "#                     E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "#                 else:\n",
    "#                     E[e_curr] = 1\n",
    "#         E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "#         return E_Filtered\n",
    "# #     first = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "# #     print(first)\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "    \n",
    "#     count = 0\n",
    "#     for edge in res:\n",
    "#         count += len(edge.keys())\n",
    "#     print(count)\n",
    "# #     print(res[(49008, 56213)])\n",
    "    logging.info('End 4')\n",
    "#     return res?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e35d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|colorId|color|\n",
      "+---+-------+-----+\n",
      "|  0|      0|  red|\n",
      "|  1|      0|  red|\n",
      "|  2|      0|  red|\n",
      "|  3|      0|  red|\n",
      "|  4|      0|  red|\n",
      "|  5|      0|  red|\n",
      "|  6|      0|  red|\n",
      "|  7|      0|  red|\n",
      "|  8|      0|  red|\n",
      "|  9|      0|  red|\n",
      "| 10|      0|  red|\n",
      "| 11|      0|  red|\n",
      "| 12|      0|  red|\n",
      "| 13|      0|  red|\n",
      "| 14|      0|  red|\n",
      "| 15|      0|  red|\n",
      "| 16|      0|  red|\n",
      "| 17|      0|  red|\n",
      "| 18|      0|  red|\n",
      "| 19|      0|  red|\n",
      "+---+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+----------+\n",
      "|  src|  dst|numOfLmers|\n",
      "+-----+-----+----------+\n",
      "|  390|34894|        46|\n",
      "|  390|41175|        46|\n",
      "|34894|41175|        53|\n",
      "|  391|34566|        30|\n",
      "|  391|40346|        30|\n",
      "|34566|40346|        66|\n",
      "|  392|25279|        20|\n",
      "|  392|24117|        44|\n",
      "|25279|24117|        73|\n",
      "|  392| 5907|        40|\n",
      "|24117| 5907|        34|\n",
      "|  393|40975|        42|\n",
      "|17347|40975|        22|\n",
      "|17347| 3568|        43|\n",
      "|41630|23664|        43|\n",
      "|41630|10503|        41|\n",
      "|23664|10503|        34|\n",
      "|  394|17759|        37|\n",
      "|  394|11374|        40|\n",
      "|17759|11374|        33|\n",
      "+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-------+-----+---------+\n",
      "| id|colorId|color|component|\n",
      "+---+-------+-----+---------+\n",
      "|  0|      0|  red|        0|\n",
      "|  1|      0|  red|        1|\n",
      "|  2|      0|  red|        0|\n",
      "|  3|      0|  red|        3|\n",
      "|  4|      0|  red|        4|\n",
      "|  5|      0|  red|        5|\n",
      "|  6|      0|  red|        6|\n",
      "|  7|      0|  red|        4|\n",
      "|  8|      0|  red|        8|\n",
      "|  9|      0|  red|        9|\n",
      "| 10|      0|  red|       10|\n",
      "| 11|      0|  red|        6|\n",
      "| 12|      0|  red|       12|\n",
      "| 13|      0|  red|       13|\n",
      "| 14|      0|  red|       14|\n",
      "| 15|      0|  red|       14|\n",
      "| 16|      0|  red|       16|\n",
      "| 17|      0|  red|       17|\n",
      "| 18|      0|  red|       18|\n",
      "| 19|      0|  red|       19|\n",
      "+---+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def build_overlap_graph( reads, spark ):\n",
    "\n",
    "edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())\n",
    "\n",
    "readsRDD = spark.sparkContext.parallelize(enumerate(reads)).repartition(40).cache()\n",
    "\n",
    "#     build_dict_origin()\n",
    "# G = build_dict_spark_map(readsRDD,spark)\n",
    "\n",
    "G.vertices.show()\n",
    "G.edges.show()\n",
    "\n",
    "logging.info('Connected Components Algorithm ...')\n",
    "sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "CC = G.connectedComponents()\n",
    "\n",
    "CC.show()\n",
    "    \n",
    "#     lmers_dict_3 = build_dict_spark_foreach(readsRDD)\n",
    "#     build_dict_spark_mapPartition(readsRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c62cd1ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2504951288.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [34]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Phân tích connected componennt\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Phân tích connected componennt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e64e50",
   "metadata": {},
   "source": [
    "CC.groupBy('component').count().orderBy('component', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4a3a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "# #     sg = spark.createDataFrame\n",
    "#     edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())\n",
    "# #     lmers_dict_3 = sc.accumulator({}, DictParam())\n",
    "# #     lmers_dict_4 = sc.accumulator({}, DictParam())\n",
    "# #     lmers_dict = sc.accumulator({}, DictParam())\n",
    "# #     gc.collect()\n",
    "#     logger.info('Start')\n",
    "# #     dict_test = build_overlap_graph(reads, sc)\n",
    "#     build_overlap_graph(reads, spark)\n",
    "#     logger.info('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9ecb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(lmers_dict_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e97277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_test['ATAAATACCTTCATTTAATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39e797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
