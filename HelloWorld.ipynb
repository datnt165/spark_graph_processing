{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81750b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biopython\n",
    "# !pip install findspark\n",
    "# !pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c800171a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphframes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccumulators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AccumulatorParam\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphframes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqIO\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphframes'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from graphframes import *\n",
    "from Bio import SeqIO\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc3cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import itertools as it\n",
    "import gc\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "from collections import namedtuple\n",
    "date_strftime_format = '%Y-%m-%y %H:%M:%S'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=date_strftime_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8fa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log4j:\n",
    "    def __init__(self, spark):\n",
    "        root_class = \"guru.learningjournal.spark.examples\"\n",
    "        conf = spark.sparkContext.getConf()\n",
    "        app_name = conf.get(\"spark.app.name\")\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        self.logger = log4j.LogManager.getLogger(root_class + '.' + app_name)\n",
    "    \n",
    "    def warn(self, message):\n",
    "        self.logger.warn(message)\n",
    "        logging.warn(message)\n",
    "        \n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "        logging.info(message)\n",
    "    \n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "        logging.error(message)\n",
    "    \n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "        logging.debug(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1948ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_read(read):\n",
    "    # Return sequence and label\n",
    "    z = re.split('[|={,]+', read.description)\n",
    "    return read.seq, z[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d688ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_reads(filename, type='fasta'):\n",
    "    try:\n",
    "        seqs = list(SeqIO.parse(filename, type))\n",
    "\n",
    "        reads = []\n",
    "        labels = []\n",
    "\n",
    "        # Detect for paired-end or single-end reads\n",
    "        # If the id of two first reads are different (e.g.: .1 and .2), they are paired-end reads\n",
    "        is_paired_end = False\n",
    "        if len(seqs) > 2 and seqs[0].id[-1:] != seqs[1].id[-1:]:\n",
    "            is_paired_end = True\n",
    "\n",
    "        label_list = dict()\n",
    "        label_index = 0\n",
    "        for i in range(0, len(seqs), 2 if is_paired_end else 1):\n",
    "            read, label = format_read(seqs[i])\n",
    "            if is_paired_end:\n",
    "                read2, label2 = format_read(seqs[i + 1])\n",
    "                read += read2\n",
    "            reads += [str(read)]\n",
    "        \n",
    "            # Create labels\n",
    "            if label not in label_list:\n",
    "                label_list[label] = label_index\n",
    "                label_index += 1\n",
    "            labels.append(label_list[label])\n",
    "        \n",
    "        del seqs\n",
    "        return reads, labels\n",
    "    except:\n",
    "        print('Error when loading file {} '.format(filename))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fe59f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325989358\n",
      "344204770\n"
     ]
    }
   ],
   "source": [
    "reads, labels = load_meta_reads('data/S1.fna', type='fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fdb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_MER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i].append(value2[i])\n",
    "            else:\n",
    "                value1[i] = [value2[i]]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd415f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEdgeParam(AccumulatorParam):\n",
    "    def zero(self,  value = \"\"):\n",
    "        return dict()\n",
    "\n",
    "    def addInPlace(self, value1, value2):\n",
    "        for i in value2.keys():\n",
    "            if i in value1:\n",
    "                value1[i] += value2[i]\n",
    "            else:\n",
    "                value1[i] = value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb676b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_origin():\n",
    "    logging.info('Start 1')\n",
    "    lmers_dict = dict()\n",
    "    for idx, r in enumerate(reads):\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            if lmer in lmers_dict:\n",
    "                lmers_dict[lmer] += [idx]\n",
    "            else:\n",
    "                lmers_dict[lmer] = [idx]\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    print('Adding nodes...')\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "                6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "                12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    for i in range(0, len(labels)):\n",
    "        G.add_node(i, label=labels[i], color=color_map[labels[i]])\n",
    "\n",
    "    print('Adding edges...')\n",
    "    for kv in E_Filtered.items():\n",
    "        G.add_edge(kv[0][0], kv[0][1], weight=kv[1])\n",
    "    print('Graph constructed!')\n",
    "    logging.info('End 1')\n",
    "    return G\n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "#     print(len(E_Filtered.keys()))\n",
    "#     print(lmers_dict[ATAAATACCTTCATTTAATA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b84492",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1779727059.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    lmers_dict =l ist()\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def build_dict_spark_map(readsRDD, spark):\n",
    "    logging.info('Start 2')\n",
    "    def create_lmers_pos(tuple):\n",
    "        idx, r = tuple\n",
    "        lmers_dict =l ist()\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict.append((lmer, idx))\n",
    "#         print(lmers_dict)\n",
    "        return lmers_dict\n",
    "    def create_edge(x):\n",
    "        lmer, idx = x\n",
    "#         print(lmer, idx)\n",
    "        global edge_dict\n",
    "        E=dict()\n",
    "        for e in it.combinations(idx,2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "                if e_curr in E:\n",
    "                    E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "                else:\n",
    "                    E[e_curr] = 1\n",
    "        edge_dict += E\n",
    "    readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).groupByKey().mapValues(list).filter(lambda x: len(x[1]) > 2).coalesce(20).foreach(create_edge)\n",
    "    # Step 1: map\n",
    "        # ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]; ['asdasd': 2, 'asdasdgg': 3, 'asdasd': 4]\n",
    "    # Step 3: flat\n",
    "        # ['asdasd': [2, 4, 90000], 'asdasdgg': 3]\n",
    "    #     res = readsRDD.map(create_lmers_pos).flatMap(lambda x: [i for i in x]).reduceByKey(lambda x,y: x.append(y)).count()\n",
    "    global edge_dict\n",
    "    E = edge_dict.value\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    print(len(E_Filtered.keys()))\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet',\n",
    "            6: 'black', 7: 'grey', 8: 'sienna', 9: 'wheat', 10: 'olive', 11: 'lightgreen',\n",
    "            12: 'cyan', 13: 'slategray', 14: 'navy', 15: 'hotpink'}\n",
    "    vertices = spark.createDataFrame([(i, labels[i], color_map[labels[i]]) for i in range(0, len(labels))])\n",
    "    edges = spark.createDataFrame([(kv[0][0], kv[0][1], kv[1])] for kv in E_Filtered.items())\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    logging.info('End 2')\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45103f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_foreach(readsRDD):\n",
    "    logging.info('Start 3')\n",
    "    def create_dict_foreach(tuple):\n",
    "        idx, r = tuple\n",
    "        global lmers_dict_3\n",
    "        for j in range(0,len(r)-L_MER+1):\n",
    "            lmer = r[j:j+L_MER]\n",
    "            lmers_dict_3 += {lmer: idx}\n",
    "    readsRDD.foreach(create_dict_foreach)\n",
    "    global lmers_dict_3\n",
    "    res = lmers_dict_3.value\n",
    "#     print(res['ATAATTGGCAAGTGTTTTAG'])\n",
    "    print(len(res.keys()))\n",
    "    logging.info('End 3')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b23bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_spark_mapPartition(readsRDD):\n",
    "    logging.info('Start 4')\n",
    "    def create_dict_mapPartition(partitionData):\n",
    "        lmers_dict = dict()\n",
    "        for idx, r in [*partitionData]:\n",
    "            for j in range(0,len(r)-L_MER+1):\n",
    "                lmer = r[j:j+L_MER]\n",
    "                if lmer in lmers_dict:\n",
    "                    lmers_dict[lmer] += [idx]\n",
    "                else:\n",
    "                    lmers_dict[lmer] = [idx]\n",
    "        yield lmers_dict\n",
    "    def merge_dict(x,y):\n",
    "        for i in y.keys():\n",
    "            if i in x:\n",
    "                x[i] += y[i] \n",
    "            else:\n",
    "                x[i] = y[i]\n",
    "        return x\n",
    "\n",
    "    lmers_dict = readsRDD.mapPartitions(create_dict_mapPartition).reduce(lambda x, y: merge_dict(x,y))\n",
    "    logging.warning('Processing 1')\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "            if e_curr in E:\n",
    "                E[e_curr] += 1\n",
    "            else:\n",
    "                E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "    \n",
    "#     print(E_Filtered[(0, 29033)])\n",
    "    print(len(E_Filtered.keys()))\n",
    "    \n",
    "#     logging.warning('Processing 2')\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).collect()\n",
    "#     count = 0\n",
    "#     for dict1 in res:\n",
    "#         count += len(dict1.keys())\n",
    "#     print(count)\n",
    "#     logging.warning('End Processing 2')\n",
    "#     def create_edge(dictionary):\n",
    "#         E  = dict()\n",
    "#         for lmer in dictionary:\n",
    "#             for e in it.combinations(dictionary[lmer],2):\n",
    "#                 if e[0]!=e[1]:\n",
    "#                     e_curr=(e[0],e[1])\n",
    "#                 if e_curr in E:\n",
    "#                     E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "#                 else:\n",
    "#                     E[e_curr] = 1\n",
    "#         E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= 20}\n",
    "#         return E_Filtered\n",
    "# #     first = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "# #     print(first)\n",
    "#     res = readsRDD.mapPartitions(create_dict_mapPartition).map(create_edge).collect()\n",
    "    \n",
    "#     count = 0\n",
    "#     for edge in res:\n",
    "#         count += len(edge.keys())\n",
    "#     print(count)\n",
    "# #     print(res[(49008, 56213)])\n",
    "    logging.info('End 4')\n",
    "#     return res?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e35d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_overlap_graph( reads, spark ):\n",
    "    # Create hash table with q-mers are keys\n",
    "    print(\"Building hash table...\")\n",
    "\n",
    "    readsRDD = spark.sparkContext.parallelize(enumerate(reads)).repartition(40).cache()\n",
    "    \n",
    "#     build_dict_origin()\n",
    "    build_dict_spark_map(readsRDD,spark)\n",
    "#     lmers_dict_3 = build_dict_spark_foreach(readsRDD)\n",
    "#     build_dict_spark_mapPartition(readsRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4a3a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:16 Start\n",
      "Building hash table...\n",
      "2022-11-22 16:34:16 Start 2\n",
      "563290\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GraphFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     dict_test = build_overlap_graph(reads, sc)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mbuild_overlap_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     gc.collect()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mbuild_overlap_graph\u001b[1;34m(reads, spark)\u001b[0m\n\u001b[0;32m      5\u001b[0m     readsRDD \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(\u001b[38;5;28menumerate\u001b[39m(reads))\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m40\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     build_dict_origin()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mbuild_dict_spark_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadsRDD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mbuild_dict_spark_map\u001b[1;34m(readsRDD, spark)\u001b[0m\n\u001b[0;32m     33\u001b[0m vertices \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(i, labels[i], color_map[labels[i]]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(labels))])\n\u001b[0;32m     34\u001b[0m edges \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(kv[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], kv[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], kv[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m kv \u001b[38;5;129;01min\u001b[39;00m E_Filtered\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m---> 35\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mGraphFrame\u001b[49m(vertices, edges)\n\u001b[0;32m     36\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GraphFrame' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('Hello Spark').master('local[*]').getOrCreate()\n",
    "    logger = Log4j(spark)\n",
    "#     sg = spark.createDataFrame\n",
    "#     edge_dict = spark.sparkContext.accumulator({}, DictEdgeParam())\n",
    "#     lmers_dict_3 = sc.accumulator({}, DictParam())\n",
    "#     lmers_dict_4 = sc.accumulator({}, DictParam())\n",
    "#     lmers_dict = sc.accumulator({}, DictParam())\n",
    "#     gc.collect()\n",
    "    logger.info('Start')\n",
    "#     dict_test = build_overlap_graph(reads, sc)\n",
    "    build_overlap_graph(reads, spark)\n",
    "#     gc.collect()\n",
    "    logger.info('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ecb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(lmers_dict_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e97277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_test['ATAAATACCTTCATTTAATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39e797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
